Model Name,Provider,Dataset,MMLU-Pro,Intelligence Index
gpt-oss-20B (low),OpenAI,OpenAI training dataset,71.80%,44.30%
gpt-oss-120B (high),OpenAI,OpenAI training dataset,80.80%,60.50%
gpt-oss-20B (high),OpenAI,OpenAI training dataset,74.80%,52.40%
gpt-oss-120B (low),OpenAI,OpenAI training dataset,77.50%,47.50%
GPT-5.1 (Non-reasoning),OpenAI,OpenAI training dataset,80.10%,42.90%
GPT-5 (low),OpenAI,OpenAI training dataset,86.00%,61.80%
GPT-5 mini (high),OpenAI,OpenAI training dataset,83.70%,64.30%
GPT-5 mini (minimal),OpenAI,OpenAI training dataset,77.50%,41.60%
GPT-5 (high),OpenAI,OpenAI training dataset,87.10%,68.50%
Grok-1,xAI,xAI training dataset,N/A,18.20%
GPT-5 (minimal),OpenAI,OpenAI training dataset,80.60%,43.50%
GPT-5 (medium),OpenAI,OpenAI training dataset,86.70%,66.40%
o3,OpenAI,OpenAI training dataset,85.30%,65.50%
GPT-5 nano (high),OpenAI,OpenAI training dataset,78.00%,51.00%
GPT-5 (ChatGPT),OpenAI,OpenAI training dataset,82.00%,41.80%
GPT-5 Codex (high),OpenAI,OpenAI training dataset,86.50%,68.50%
GPT-5 nano (minimal),OpenAI,OpenAI training dataset,55.60%,29.10%
GPT-5 nano (medium),OpenAI,OpenAI training dataset,77.20%,49.30%
GPT-5 mini (medium),OpenAI,OpenAI training dataset,82.80%,60.80%
GPT-5.1 (high),OpenAI,OpenAI training dataset,87.00%,69.70%
Llama 3.3 Instruct 70B,Meta,Meta training dataset,71.30%,27.90%
Llama 3.1 Instruct 405B,Meta,Meta training dataset,73.20%,28.10%
Llama 3.2 Instruct 90B (Vision),Meta,Meta training dataset,67.10%,18.90%
Llama 3.2 Instruct 11B (Vision),Meta,Meta training dataset,46.40%,15.50%
Llama 4 Scout,Meta,Meta training dataset,75.20%,28.10%
Llama 4 Maverick,Meta,Meta training dataset,80.90%,35.80%
Gemini 3 Pro Preview,Google,Google training dataset,89.80%,72.80%
Gemini 2.5 Flash-Lite Preview (Sep '25) (Non-reasoning),Google,Google training dataset,79.60%,41.60%
Gemma 3 1B Instruct,Google,Google training dataset,13.50%,6.80%
Gemma 3n E4B Instruct,Google,Google training dataset,48.80%,15.50%
Gemini 2.5 Flash Preview (Sep '25) (Reasoning),Google,Google training dataset,84.20%,54.40%
Gemma 3 27B Instruct,Google,Google training dataset,66.90%,22.10%
Gemma 3 270M,Google,Google training dataset,5.50%,5.60%
Gemini 2.5 Pro,Google,Google training dataset,86.20%,59.60%
Gemini 2.5 Flash-Lite Preview (Sep '25) (Reasoning),Google,Google training dataset,80.80%,47.90%
Gemma 3n E2B Instruct,Google,Google training dataset,37.80%,11.30%
Gemma 3 12B Instruct,Google,Google training dataset,59.50%,20.40%
Gemma 3 4B Instruct,Google,Google training dataset,41.70%,14.70%
Gemini 2.5 Flash Preview (Sep '25) (Non-reasoning),Google,Google training dataset,83.60%,46.70%
Ministral 8B,Mistral,Mistral training dataset,38.90%,12.40%
Ministral 3B,Mistral,Mistral training dataset,33.90%,10.90%
Mistral Medium 3.1,Mistral,Mistral training dataset,68.30%,35.40%
Devstral Small (Jul '25),Mistral,Mistral training dataset,62.20%,27.20%
Codestral (Jan '25),Mistral,Mistral training dataset,44.60%,20.10%
Magistral Medium 1.2,Mistral,Mistral training dataset,81.50%,52.00%
Magistral Small 1.2,Mistral,Mistral training dataset,76.80%,43.00%
Devstral Medium,Mistral,Mistral training dataset,70.80%,27.90%
Mistral Small 3.2,Mistral,Mistral training dataset,68.10%,29.10%
DeepSeek R1 Distill Llama 70B,DeepSeek,DeepSeek training dataset,79.50%,29.90%
DeepSeek V3.2 Exp (Reasoning),DeepSeek,DeepSeek training dataset,85.00%,56.90%
DeepSeek R1 0528 Qwen3 8B,DeepSeek,DeepSeek training dataset,73.90%,31.00%
DeepSeek V3.2 Exp (Non-reasoning),DeepSeek,DeepSeek training dataset,83.60%,46.30%
DeepSeek R1 0528 (May '25),DeepSeek,DeepSeek training dataset,84.90%,52.00%
DeepSeek V3.1 Terminus (Non-reasoning),DeepSeek,DeepSeek training dataset,83.60%,45.70%
DeepSeek V3.1 Terminus (Reasoning),DeepSeek,DeepSeek training dataset,85.10%,57.70%
R1 1776,Perplexity,Perplexity training dataset,N/A,19.10%
Grok 4 Fast (Non-reasoning),xAI,xAI training dataset,73.00%,38.60%
Grok Code Fast 1,xAI,xAI training dataset,79.30%,48.60%
Grok 4.1 Fast (Reasoning),xAI,xAI training dataset,85.40%,64.10%
Grok 4 Fast (Reasoning),xAI,xAI training dataset,85.00%,60.30%
Grok 4,xAI,xAI training dataset,86.60%,65.30%
Grok 3 mini Reasoning (high),xAI,xAI training dataset,82.80%,57.10%
Phi-4,Microsoft Azure,Microsoft Azure training dataset,71.40%,22.70%
Phi-4 Mini Instruct,Microsoft Azure,Microsoft Azure training dataset,46.50%,15.70%
Phi-4 Multimodal Instruct,Microsoft Azure,Microsoft Azure training dataset,48.50%,12.40%
LFM2 1.2B,Liquid AI,Liquid AI training dataset,25.70%,9.70%
LFM2 2.6B,Liquid AI,Liquid AI training dataset,29.80%,11.80%
LFM2 8B A1B,Liquid AI,Liquid AI training dataset,50.50%,17.40%
Solar Pro 2 (Reasoning),Upstage,Upstage training dataset,80.50%,37.70%
Solar Pro 2 (Non-reasoning),Upstage,Upstage training dataset,75.00%,30.20%
MiniMax-Text-01,MiniMax,MiniMax training dataset,75.90%,28.30%
MiniMax-M2,MiniMax,MiniMax training dataset,82.00%,61.40%
Llama 3.1 Nemotron Instruct 70B,NVIDIA,NVIDIA training dataset,69.00%,23.60%
Llama Nemotron Super 49B v1.5 (Non-reasoning),NVIDIA,NVIDIA training dataset,69.20%,26.60%
Llama 3.3 Nemotron Super 49B v1 (Reasoning),NVIDIA,NVIDIA training dataset,78.50%,35.50%
Llama 3.3 Nemotron Super 49B v1 (Non-reasoning),NVIDIA,NVIDIA training dataset,69.80%,25.90%
Llama 3.1 Nemotron Ultra 253B v1 (Reasoning),NVIDIA,NVIDIA training dataset,82.50%,38.50%
NVIDIA Nemotron Nano 9B V2 (Reasoning),NVIDIA,NVIDIA training dataset,74.20%,37.20%
NVIDIA Nemotron Nano 9B V2 (Non-reasoning),NVIDIA,NVIDIA training dataset,73.90%,36.10%
Llama 3.1 Nemotron Nano 4B v1.1 (Reasoning),NVIDIA,NVIDIA training dataset,55.60%,26.10%
Llama Nemotron Super 49B v1.5 (Reasoning),NVIDIA,NVIDIA training dataset,81.40%,45.20%
Kimi K2 Thinking,Moonshot AI,Moonshot AI training dataset,84.80%,67.00%
Kimi K2 0905,Moonshot AI,Moonshot AI training dataset,81.90%,50.40%
Kimi Linear 48B A3B Instruct,Moonshot AI,Moonshot AI training dataset,58.50%,N/A
OLMo 2 32B,Allen Institute for AI,Allen Institute for AI training dataset,51.10%,14.40%
OLMo 2 7B,Allen Institute for AI,Allen Institute for AI training dataset,28.20%,9.50%
Molmo 7B-D,Allen Institute for AI,Allen Institute for AI training dataset,37.10%,9.30%
Granite 4.0 H 350M,IBM,IBM training dataset,12.70%,8.20%
Granite 4.0 350M,IBM,IBM training dataset,12.40%,7.70%
Granite 4.0 H 1B,IBM,IBM training dataset,27.70%,13.70%
Granite 4.0 1B,IBM,IBM training dataset,32.50%,13.30%
Granite 4.0 H Small,IBM,IBM training dataset,62.40%,22.70%
Granite 4.0 Micro,IBM,IBM training dataset,44.70%,16.20%
DeepHermes 3 - Mistral 24B Preview (Non-reasoning),Nous Research,Nous Research training dataset,58.00%,15.50%
Hermes 4 - Llama-3.1 70B (Reasoning),Nous Research,Nous Research training dataset,81.10%,39.20%
DeepHermes 3 - Llama-3.1 8B Preview (Non-reasoning),Nous Research,Nous Research training dataset,36.50%,1.80%
Hermes 4 - Llama-3.1 405B (Non-reasoning),Nous Research,Nous Research training dataset,72.90%,32.60%
Hermes 4 - Llama-3.1 70B (Non-reasoning),Nous Research,Nous Research training dataset,66.40%,23.80%
Hermes 4 - Llama-3.1 405B (Reasoning),Nous Research,Nous Research training dataset,82.90%,41.60%
EXAONE 4.0 32B (Non-reasoning),LG AI Research,LG AI Research training dataset,76.80%,30.30%
Exaone 4.0 1.2B (Non-reasoning),LG AI Research,LG AI Research training dataset,50.00%,20.50%
EXAONE 4.0 32B (Reasoning),LG AI Research,LG AI Research training dataset,81.80%,42.60%
Exaone 4.0 1.2B (Reasoning),LG AI Research,LG AI Research training dataset,58.80%,26.70%
ERNIE 4.5 300B A47B,Baidu,Baidu training dataset,77.60%,32.90%
Cogito v2.1 (Reasoning),Deep Cogito,Deep Cogito training dataset,84.90%,N/A
GLM-4.5-Air,Z AI,Z AI training dataset,81.50%,48.80%
GLM-4.6 (Reasoning),Z AI,Z AI training dataset,82.90%,56.00%
GLM-4.5V (Reasoning),Z AI,Z AI training dataset,78.80%,37.00%
GLM-4.6 (Non-reasoning),Z AI,Z AI training dataset,78.40%,44.70%
GLM-4.5V (Non-reasoning),Z AI,Z AI training dataset,75.10%,26.00%
Aya Expanse 32B,Cohere,Cohere training dataset,37.70%,13.60%
Aya Expanse 8B,Cohere,Cohere training dataset,31.20%,10.00%
Command A,Cohere,Cohere training dataset,71.20%,26.90%
Apriel-v1.5-15B-Thinker,ServiceNow,ServiceNow training dataset,77.30%,51.60%
Jamba 1.7 Large,AI21 Labs,AI21 Labs training dataset,57.70%,20.80%
Jamba Reasoning 3B,AI21 Labs,AI21 Labs training dataset,57.70%,20.90%
Jamba 1.7 Mini,AI21 Labs,AI21 Labs training dataset,38.80%,14.80%
Qwen3 Coder 480B A35B Instruct,Alibaba,Alibaba training dataset,78.80%,42.30%
Qwen3 4B 2507 Instruct,Alibaba,Alibaba training dataset,67.20%,30.40%
Qwen3 4B 2507 (Reasoning),Alibaba,Alibaba training dataset,74.30%,43.40%
Qwen3 235B A22B 2507 Instruct,Alibaba,Alibaba training dataset,82.80%,45.30%
Qwen3 Coder 30B A3B Instruct,Alibaba,Alibaba training dataset,70.60%,33.40%
Qwen3 VL 32B (Reasoning),Alibaba,Alibaba training dataset,81.80%,51.90%
Qwen3 VL 32B Instruct,Alibaba,Alibaba training dataset,79.10%,41.00%
Qwen3 235B A22B 2507 (Reasoning),Alibaba,Alibaba training dataset,84.30%,57.50%
Qwen3 Next 80B A3B Instruct,Alibaba,Alibaba training dataset,81.90%,44.80%
Qwen3 Next 80B A3B (Reasoning),Alibaba,Alibaba training dataset,82.40%,54.30%
Qwen3 30B A3B 2507 (Reasoning),Alibaba,Alibaba training dataset,80.50%,46.40%
Qwen3 30B A3B 2507 Instruct,Alibaba,Alibaba training dataset,77.70%,37.00%
Qwen3 Omni 30B A3B (Reasoning),Alibaba,Alibaba training dataset,79.20%,40.00%
Qwen3 VL 235B A22B Instruct,Alibaba,Alibaba training dataset,82.30%,44.10%
Qwen3 Omni 30B A3B Instruct,Alibaba,Alibaba training dataset,72.50%,30.20%
Llama 65B,Meta,Meta training dataset,N/A,100.00%
Qwen3 VL 30B A3B Instruct,Alibaba,Alibaba training dataset,76.40%,38.50%
Qwen3 VL 30B A3B (Reasoning),Alibaba,Alibaba training dataset,80.70%,45.30%
Qwen3 Max,Alibaba,Alibaba training dataset,84.10%,55.10%
Qwen3 VL 235B A22B (Reasoning),Alibaba,Alibaba training dataset,83.60%,54.40%
Qwen3 Max Thinking,Alibaba,Alibaba training dataset,82.40%,55.80%
Qwen3 VL 4B Instruct,Alibaba,Alibaba training dataset,63.40%,25.20%
Qwen3 VL 8B Instruct,Alibaba,Alibaba training dataset,68.60%,27.10%
Qwen3 VL 8B (Reasoning),Alibaba,Alibaba training dataset,74.90%,32.10%
Qwen3 VL 4B (Reasoning),Alibaba,Alibaba training dataset,70.00%,27.30%
Ring-flash-2.0,InclusionAI,InclusionAI training dataset,79.30%,39.50%
Ring-1T,InclusionAI,InclusionAI training dataset,80.60%,41.80%
Ling-mini-2.0,InclusionAI,InclusionAI training dataset,67.10%,27.80%
Ling-flash-2.0,InclusionAI,InclusionAI training dataset,77.70%,38.30%
Ling-1T,InclusionAI,InclusionAI training dataset,82.20%,44.80%
Seed-OSS-36B-Instruct,ByteDance Seed,ByteDance Seed training dataset,81.50%,51.60%
Doubao Seed Code,ByteDance Seed,ByteDance Seed training dataset,85.40%,57.10%
o1,OpenAI,OpenAI training dataset,84.10%,47.20%
o1-preview,OpenAI,OpenAI training dataset,N/A,44.90%
o1-mini,OpenAI,OpenAI training dataset,74.20%,39.20%
GPT-4o (Aug '24),OpenAI,OpenAI training dataset,N/A,29.00%
Qwen Chat 14B,Alibaba,Alibaba training dataset,N/A,100.00%
GPT-4o (May '24),OpenAI,OpenAI training dataset,74.00%,26.30%
GPT-4 Turbo,OpenAI,OpenAI training dataset,69.40%,24.20%
GPT-4o (Nov '24),OpenAI,OpenAI training dataset,74.80%,27.00%
GPT-4o mini,OpenAI,OpenAI training dataset,64.80%,21.20%
GPT-3.5 Turbo,OpenAI,OpenAI training dataset,46.20%,8.30%
GPT-4.1,OpenAI,OpenAI training dataset,80.60%,43.40%
o3-mini (high),OpenAI,OpenAI training dataset,80.20%,50.80%
GPT-4.1 nano,OpenAI,OpenAI training dataset,65.70%,27.30%
GPT-4.1 mini,OpenAI,OpenAI training dataset,78.10%,42.50%
o4-mini (high),OpenAI,OpenAI training dataset,83.20%,59.60%
GPT-4.5 (Preview),OpenAI,OpenAI training dataset,N/A,38.40%
GPT-4o (ChatGPT),OpenAI,OpenAI training dataset,77.30%,25.30%
"GPT-4o (March 2025, chatgpt-4o-latest)",OpenAI,OpenAI training dataset,80.30%,35.60%
o1-pro,OpenAI,OpenAI training dataset,N/A,48.00%
o3-mini,OpenAI,OpenAI training dataset,79.10%,48.10%
GPT-4,OpenAI,OpenAI training dataset,N/A,21.50%
o3-pro,OpenAI,OpenAI training dataset,N/A,65.30%
Llama 3.1 Instruct 70B,Meta,Meta training dataset,67.60%,22.60%
Llama 3.1 Instruct 8B,Meta,Meta training dataset,47.60%,16.90%
Llama 3.2 Instruct 3B,Meta,Meta training dataset,34.70%,11.20%
Llama 3 Instruct 70B,Meta,Meta training dataset,57.40%,13.00%
Llama 3 Instruct 8B,Meta,Meta training dataset,40.50%,7.00%
Llama 3.2 Instruct 1B,Meta,Meta training dataset,20.00%,8.90%
Llama 2 Chat 70B,Meta,Meta training dataset,40.60%,5.60%
Llama 2 Chat 13B,Meta,Meta training dataset,40.60%,5.50%
Llama 2 Chat 7B,Meta,Meta training dataset,16.40%,11.30%
Gemini 2.0 Pro Experimental (Feb '25),Google,Google training dataset,80.50%,34.60%
Gemini 2.0 Flash (experimental),Google,Google training dataset,78.20%,31.80%
Gemini 1.5 Pro (Sep '24),Google,Google training dataset,75.00%,30.00%
Gemini 2.0 Flash-Lite (Preview),Google,Google training dataset,N/A,26.30%
Gemini 2.0 Flash (Feb '25),Google,Google training dataset,77.90%,33.60%
Gemini 1.5 Flash (Sep '24),Google,Google training dataset,68.00%,24.40%
Gemma 2 27B,Google,Google training dataset,57.50%,17.20%
Gemma 2 9B,Google,Google training dataset,49.50%,7.80%
Gemini 1.5 Flash-8B,Google,Google training dataset,56.90%,16.30%
Gemini 2.0 Flash Thinking Experimental (Jan '25),Google,Google training dataset,79.80%,37.70%
Gemini 2.5 Flash-Lite (Reasoning),Google,Google training dataset,75.90%,40.10%
Gemini 1.0 Pro,Google,Google training dataset,43.10%,6.20%
Gemini 1.5 Pro (May '24),Google,Google training dataset,65.70%,19.20%
Gemini 1.0 Ultra,Google,Google training dataset,N/A,12.80%
Gemini 2.5 Flash Preview (Non-reasoning),Google,Google training dataset,78.30%,34.10%
Gemini 1.5 Flash (May '24),Google,Google training dataset,57.40%,14.00%
Gemini 2.5 Pro Preview (May' 25),Google,Google training dataset,83.70%,53.20%
Gemini 2.5 Flash (Non-reasoning),Google,Google training dataset,80.90%,40.40%
Gemini 2.5 Flash Preview (Reasoning),Google,Google training dataset,80.00%,45.80%
Gemini 2.0 Flash-Lite (Feb '25),Google,Google training dataset,72.40%,26.80%
Gemini 2.0 Flash Thinking Experimental (Dec '24),Google,Google training dataset,N/A,20.20%
Gemini 2.5 Flash (Reasoning),Google,Google training dataset,83.20%,51.20%
Gemini 2.5 Flash-Lite (Non-reasoning),Google,Google training dataset,72.40%,30.10%
Gemma 3n E4B Instruct Preview (May '25),Google,Google training dataset,48.30%,12.50%
Gemini 2.5 Pro Preview (Mar' 25),Google,Google training dataset,85.80%,54.10%
PALM-2,Google,Google training dataset,N/A,6.60%
Mistral Large 2 (Nov '24),Mistral,Mistral training dataset,69.70%,26.80%
Mistral Large 2 (Jul '24),Mistral,Mistral training dataset,68.30%,22.30%
Pixtral Large,Mistral,Mistral training dataset,70.10%,25.00%
Mistral Small 3,Mistral,Mistral training dataset,65.20%,21.20%
Mistral Small (Sep '24),Mistral,Mistral training dataset,52.90%,13.00%
Mixtral 8x22B Instruct,Mistral,Mistral training dataset,53.70%,11.70%
Mistral Small (Feb '24),Mistral,Mistral training dataset,41.90%,8.50%
Mistral Large (Feb '24),Mistral,Mistral training dataset,51.50%,11.90%
Pixtral 12B (2409),Mistral,Mistral training dataset,47.30%,8.90%
Mistral NeMo,Mistral,Mistral training dataset,39.90%,5.20%
Mixtral 8x7B Instruct,Mistral,Mistral training dataset,38.70%,2.60%
Codestral-Mamba,Mistral,Mistral training dataset,20.90%,100.00%
Mistral 7B Instruct,Mistral,Mistral training dataset,24.50%,100.00%
Devstral Small (May '25),Mistral,Mistral training dataset,63.20%,19.60%
Mistral Small 3.1,Mistral,Mistral training dataset,65.90%,23.10%
Codestral (May '24),Mistral,Mistral training dataset,33.10%,6.00%
Mistral Saba,Mistral,Mistral training dataset,61.10%,19.60%
Mistral Medium,Mistral,Mistral training dataset,49.10%,8.40%
Magistral Small 1,Mistral,Mistral training dataset,74.60%,31.90%
Magistral Medium 1,Mistral,Mistral training dataset,75.30%,33.20%
Mistral Medium 3,Mistral,Mistral training dataset,76.00%,33.60%
DeepSeek R1 Distill Qwen 32B,DeepSeek,DeepSeek training dataset,73.90%,32.70%
DeepSeek V3 (Dec '24),DeepSeek,DeepSeek training dataset,75.20%,32.50%
DeepSeek R1 Distill Qwen 14B,DeepSeek,DeepSeek training dataset,74.00%,29.70%
DeepSeek-V2.5 (Dec '24),DeepSeek,DeepSeek training dataset,N/A,20.70%
DeepSeek-Coder-V2,DeepSeek,DeepSeek training dataset,N/A,14.50%
DeepSeek R1 Distill Llama 8B,DeepSeek,DeepSeek training dataset,54.30%,19.50%
DeepSeek LLM 67B Chat (V1),DeepSeek,DeepSeek training dataset,N/A,5.60%
DeepSeek R1 Distill Qwen 1.5B,DeepSeek,DeepSeek training dataset,26.90%,8.60%
DeepSeek V3.1 (Non-reasoning),DeepSeek,DeepSeek training dataset,83.30%,44.80%
DeepSeek R1 (Jan '25),DeepSeek,DeepSeek training dataset,84.40%,43.80%
DeepSeek V3.1 (Reasoning),DeepSeek,DeepSeek training dataset,85.10%,54.00%
DeepSeek V3 0324,DeepSeek,DeepSeek training dataset,81.90%,41.30%
DeepSeek Coder V2 Lite Instruct,DeepSeek,DeepSeek training dataset,42.90%,6.10%
DeepSeek-V2.5,DeepSeek,DeepSeek training dataset,N/A,20.20%
DeepSeek-V2-Chat,DeepSeek,DeepSeek training dataset,N/A,8.60%
Sonar Pro,Perplexity,Perplexity training dataset,75.50%,28.20%
Sonar,Perplexity,Perplexity training dataset,68.90%,28.80%
Sonar Reasoning Pro,Perplexity,Perplexity training dataset,N/A,46.30%
Sonar Reasoning,Perplexity,Perplexity training dataset,N/A,34.20%
Grok Beta,xAI,xAI training dataset,70.30%,23.00%
Grok 3,xAI,xAI training dataset,79.90%,45.30%
Grok 3 Reasoning Beta,xAI,xAI training dataset,N/A,41.40%
Grok 2 (Dec '24),xAI,xAI training dataset,70.90%,24.70%
OpenChat 3.5 (1210),OpenChat,OpenChat training dataset,31.00%,5.40%
Phi-3 Medium Instruct 14B,Microsoft Azure,Microsoft Azure training dataset,54.30%,14.40%
Phi-3 Mini Instruct 3.8B,Microsoft Azure,Microsoft Azure training dataset,43.50%,12.70%
LFM 40B,Liquid AI,Liquid AI training dataset,42.50%,7.30%
Solar Mini,Upstage,Upstage training dataset,N/A,18.90%
Solar Pro 2 (Preview) (Non-reasoning),Upstage,Upstage training dataset,72.50%,30.00%
Solar Pro 2 (Preview) (Reasoning),Upstage,Upstage training dataset,76.80%,36.10%
DBRX Instruct,Databricks,Databricks training dataset,39.70%,5.30%
MiniMax M1 40k,MiniMax,MiniMax training dataset,80.80%,40.00%
MiniMax M1 80k,MiniMax,MiniMax training dataset,81.60%,46.20%
Kimi K2,Moonshot AI,Moonshot AI training dataset,82.40%,48.10%
Llama 3.1 Tulu3 405B,Allen Institute for AI,Allen Institute for AI training dataset,71.60%,25.40%
Granite 3.3 8B (Non-reasoning),IBM,IBM training dataset,46.80%,15.20%
Hermes 3 - Llama-3.1 70B,Nous Research,Nous Research training dataset,57.10%,14.70%
GLM-4.5 (Reasoning),Z AI,Z AI training dataset,83.50%,51.30%
Command-R+ (Aug '24),Cohere,Cohere training dataset,42.70%,7.10%
Command-R+ (Apr '24),Cohere,Cohere training dataset,43.20%,5.50%
Command-R (Aug '24),Cohere,Cohere training dataset,33.70%,100.00%
Command-R (Mar '24),Cohere,Cohere training dataset,33.80%,100.00%
Jamba Instruct,AI21 Labs,AI21 Labs training dataset,34.30%,11.80%
Jamba 1.6 Mini,AI21 Labs,AI21 Labs training dataset,36.70%,3.30%
Jamba 1.6 Large,AI21 Labs,AI21 Labs training dataset,56.50%,14.30%
Jamba 1.5 Mini,AI21 Labs,AI21 Labs training dataset,37.10%,4.00%
Jamba 1.5 Large,AI21 Labs,AI21 Labs training dataset,57.20%,14.80%
Arctic Instruct,Snowflake,Snowflake training dataset,N/A,7.60%
Qwen2.5 Max,Alibaba,Alibaba training dataset,76.20%,30.70%
Qwen2.5 Instruct 72B,Alibaba,Alibaba training dataset,72.00%,29.00%
Qwen2.5 Coder Instruct 32B,Alibaba,Alibaba training dataset,63.50%,21.80%
Qwen2.5 Turbo,Alibaba,Alibaba training dataset,63.30%,19.10%
Qwen2 Instruct 72B,Alibaba,Alibaba training dataset,62.20%,18.10%
Qwen3 32B (Non-reasoning),Alibaba,Alibaba training dataset,72.70%,26.40%
Qwen3 4B (Non-reasoning),Alibaba,Alibaba training dataset,58.60%,20.70%
Qwen2.5 Instruct 32B,Alibaba,Alibaba training dataset,69.70%,22.90%
Qwen3 30B A3B (Reasoning),Alibaba,Alibaba training dataset,77.70%,36.70%
Qwen3 235B A22B (Reasoning),Alibaba,Alibaba training dataset,82.80%,41.70%
Qwen3 32B (Reasoning),Alibaba,Alibaba training dataset,79.80%,38.70%
Qwen3 14B (Non-reasoning),Alibaba,Alibaba training dataset,67.50%,29.20%
Qwen3 1.7B (Non-reasoning),Alibaba,Alibaba training dataset,41.10%,14.40%
Qwen3 8B (Non-reasoning),Alibaba,Alibaba training dataset,64.30%,22.90%
Qwen3 8B (Reasoning),Alibaba,Alibaba training dataset,74.30%,28.30%
QwQ 32B,Alibaba,Alibaba training dataset,76.40%,37.90%
Qwen3 235B A22B (Non-reasoning),Alibaba,Alibaba training dataset,76.20%,29.90%
QwQ 32B-Preview,Alibaba,Alibaba training dataset,64.80%,28.00%
Qwen3 4B (Reasoning),Alibaba,Alibaba training dataset,69.60%,25.60%
Qwen3 0.6B (Non-reasoning),Alibaba,Alibaba training dataset,23.10%,11.00%
Qwen3 30B A3B (Non-reasoning),Alibaba,Alibaba training dataset,71.00%,26.50%
Qwen2.5 Coder Instruct 7B ,Alibaba,Alibaba training dataset,47.30%,12.20%
Qwen3 14B (Reasoning),Alibaba,Alibaba training dataset,77.40%,36.00%
Qwen Chat 72B,Alibaba,Alibaba training dataset,N/A,7.60%
Qwen3 1.7B (Reasoning),Alibaba,Alibaba training dataset,57.00%,22.40%
Qwen3 Max (Preview),Alibaba,Alibaba training dataset,83.80%,48.50%
Qwen3 0.6B (Reasoning),Alibaba,Alibaba training dataset,34.70%,14.20%
Qwen1.5 Chat 110B,Alibaba,Alibaba training dataset,N/A,10.50%
