# CSV Files - Quick Reference

## Summary Table

| File Name | Type | Models | Columns | Purpose | Generated By |
|-----------|------|--------|---------|---------|--------------|
| **Use Case CSVs (9 files)** |
| `opensource_chatbot_conversational.csv` | Use Case | 204 | 4 | Chatbot rankings | `create_usecase_scores.py` |
| `opensource_code_completion.csv` | Use Case | 204 | 4 | Code completion rankings | `create_usecase_scores.py` |
| `opensource_code_generation_detailed.csv` | Use Case | 204 | 4 | Code generation rankings | `create_usecase_scores.py` |
| `opensource_translation.csv` | Use Case | 204 | 4 | Translation rankings | `create_usecase_scores.py` |
| `opensource_content_generation.csv` | Use Case | 204 | 4 | Content creation rankings | `create_usecase_scores.py` |
| `opensource_summarization_short.csv` | Use Case | 204 | 4 | Summarization rankings | `create_usecase_scores.py` |
| `opensource_document_analysis_rag.csv` | Use Case | 204 | 4 | RAG/document Q&A rankings | `create_usecase_scores.py` |
| `opensource_long_document_summarization.csv` | Use Case | 204 | 4 | Long doc summarization | `create_usecase_scores.py` |
| `opensource_research_legal_analysis.csv` | Use Case | 204 | 4 | Research/legal analysis | `create_usecase_scores.py` |
| **Subject-Specific CSVs (5 files)** |
| `opensource_mathematics.csv` | Subject | 204 | 7 | Math benchmarks only | `update_subject_specific_csvs.py` |
| `opensource_reasoning.csv` | Subject | 204 | 5 | Reasoning benchmarks | `update_subject_specific_csvs.py` |
| `opensource_science.csv` | Subject | 204 | 6 | Science benchmarks | `update_subject_specific_csvs.py` |
| `opensource_computer_science.csv` | Subject | 204 | 7 | Coding benchmarks | `update_subject_specific_csvs.py` |
| `opensource_general_knowledge.csv` | Subject | 204 | 5 | Knowledge benchmarks | `update_subject_specific_csvs.py` |
| **Output CSVs (variable)** |
| `best_models_*.csv` | Output | 204 | 4 | Semantic matching results | `get_best_models_semantic.py` |

## Column Structures

### Use Case CSVs
```
Model Name, Provider, Dataset, Use Case Score
```

### Subject-Specific CSVs

**Mathematics:**
```
Model Name, Provider, Dataset, Math 500, AIME, AIME 2025, Math Index
```

**Reasoning:**
```
Model Name, Provider, Dataset, AA-LCR, τ²-Bench Telecom
```

**Science:**
```
Model Name, Provider, Dataset, SciCode, GPQA Diamond, Humanity's Last Exam
```

**Computer Science:**
```
Model Name, Provider, Dataset, LiveCodeBench, IFBench, Terminal-Bench Hard, Coding Index
```

**General Knowledge:**
```
Model Name, Provider, Dataset, MMLU-Pro, Intelligence Index
```

## How Each Type is Built

### Use Case CSVs
1. Read master CSV (`opensource_all_benchmarks.csv`)
2. Apply use-case-specific weights to benchmarks
3. Calculate weighted average score
4. Sort by score (descending)
5. Export with 4 columns

### Subject-Specific CSVs
1. Read master CSV
2. Extract only relevant benchmark columns
3. Format scores as percentages
4. Export with subject-specific columns

### Best Models CSVs
1. User provides use case description
2. Semantic matching finds similar use cases
3. Load relevant use case CSV(s)
4. Combine with weighted scores if multiple matches
5. Export sorted results

## File Count Summary

- **Use Case CSVs**: 9 files
- **Subject CSVs**: 5 files
- **Output CSVs**: Variable (can be deleted)
- **Total tracked**: 14 core files

## See Also

- `CSV_FILES_EXPLAINED.md` - Detailed explanation
- `USE_CASE_METHODOLOGY.md` - How use case weights work
- `SEMANTIC_MATCHING_EXPLAINED.md` - How semantic matching works

