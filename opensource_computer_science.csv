Model Name,Provider,Dataset,LiveCodeBench,IFBench,Terminal-Bench Hard,Coding Index
gpt-oss-20B (low),OpenAI,OpenAI training dataset,65.20%,57.80%,4.30%,34.50%
gpt-oss-120B (high),OpenAI,OpenAI training dataset,87.80%,69.00%,22.00%,49.60%
gpt-oss-20B (high),OpenAI,OpenAI training dataset,77.70%,65.10%,9.90%,40.70%
gpt-oss-120B (low),OpenAI,OpenAI training dataset,70.70%,58.30%,5.00%,37.20%
GPT-5.1 (Non-reasoning),OpenAI,OpenAI training dataset,49.40%,43.20%,21.30%,35.70%
GPT-5 (low),OpenAI,OpenAI training dataset,76.30%,66.60%,24.80%,46.80%
GPT-5 mini (high),OpenAI,OpenAI training dataset,83.80%,75.40%,31.20%,51.40%
GPT-5 mini (minimal),OpenAI,OpenAI training dataset,54.50%,45.60%,13.50%,35.00%
GPT-5 (high),OpenAI,OpenAI training dataset,84.60%,73.10%,30.50%,52.70%
GPT-5 (minimal),OpenAI,OpenAI training dataset,55.80%,45.60%,17.70%,37.40%
GPT-5 (medium),OpenAI,OpenAI training dataset,70.30%,70.60%,36.20%,49.20%
o3,OpenAI,OpenAI training dataset,80.80%,71.40%,34.80%,52.20%
GPT-5 nano (high),OpenAI,OpenAI training dataset,78.90%,67.60%,11.30%,42.30%
GPT-5 (ChatGPT),OpenAI,OpenAI training dataset,54.30%,45.00%,12.10%,34.70%
GPT-5 Codex (high),OpenAI,OpenAI training dataset,84.00%,74.10%,35.50%,53.50%
GPT-5 nano (minimal),OpenAI,OpenAI training dataset,47.00%,32.50%,6.40%,27.50%
GPT-5 nano (medium),OpenAI,OpenAI training dataset,76.30%,65.90%,16.30%,42.10%
GPT-5 mini (medium),OpenAI,OpenAI training dataset,69.20%,71.20%,27.00%,45.70%
GPT-5.1 (high),OpenAI,OpenAI training dataset,86.80%,72.90%,42.60%,57.50%
Llama 3.3 Instruct 70B,Meta,Meta training dataset,28.80%,47.10%,2.80%,19.20%
Llama 3.1 Instruct 405B,Meta,Meta training dataset,30.50%,39.00%,6.40%,22.20%
Llama 3.2 Instruct 90B (Vision),Meta,Meta training dataset,21.40%,N/A,N/A,N/A
Llama 3.2 Instruct 11B (Vision),Meta,Meta training dataset,11.00%,30.40%,0.70%,7.70%
Llama 4 Scout,Meta,Meta training dataset,29.90%,39.50%,1.40%,16.10%
Llama 4 Maverick,Meta,Meta training dataset,39.70%,43.00%,6.40%,26.40%
Gemini 3 Pro Preview,Google,Google training dataset,91.70%,70.40%,39.00%,62.30%
Gemini 2.5 Flash-Lite Preview (Sep '25) (Non-reasoning),Google,Google training dataset,64.10%,41.80%,7.10%,33.20%
Gemma 3 1B Instruct,Google,Google training dataset,1.70%,19.90%,0.00%,80.00%
Gemma 3n E4B Instruct,Google,Google training dataset,14.60%,27.90%,2.10%,8.30%
Gemini 2.5 Flash Preview (Sep '25) (Reasoning),Google,Google training dataset,71.30%,52.30%,15.60%,42.50%
Gemma 3 27B Instruct,Google,Google training dataset,13.70%,31.80%,3.50%,12.80%
Gemma 3 270M,Google,Google training dataset,0.30%,12.10%,0.00%,10.00%
Gemini 2.5 Pro,Google,Google training dataset,80.10%,48.70%,24.80%,49.30%
Gemini 2.5 Flash-Lite Preview (Sep '25) (Reasoning),Google,Google training dataset,68.80%,52.60%,12.10%,36.50%
Gemma 3n E2B Instruct,Google,Google training dataset,9.50%,22.00%,0.70%,5.20%
Gemma 3 12B Instruct,Google,Google training dataset,13.70%,36.70%,0.70%,10.60%
Gemma 3 4B Instruct,Google,Google training dataset,11.20%,28.30%,0.70%,6.40%
Gemini 2.5 Flash Preview (Sep '25) (Non-reasoning),Google,Google training dataset,62.50%,43.50%,13.50%,37.80%
Ministral 8B,Mistral,Mistral training dataset,11.20%,19.30%,0.00%,7.60%
Ministral 3B,Mistral,Mistral training dataset,6.90%,20.50%,0.00%,5.40%
Mistral Medium 3.1,Mistral,Mistral training dataset,40.60%,39.80%,9.90%,28.10%
Devstral Small (Jul '25),Mistral,Mistral training dataset,25.40%,34.60%,5.70%,18.50%
Codestral (Jan '25),Mistral,Mistral training dataset,24.30%,24.80%,0.00%,16.30%
Magistral Medium 1.2,Mistral,Mistral training dataset,75.00%,43.00%,12.80%,42.30%
Magistral Small 1.2,Mistral,Mistral training dataset,72.30%,44.40%,4.30%,37.20%
Devstral Medium,Mistral,Mistral training dataset,33.70%,29.90%,8.50%,23.90%
Mistral Small 3.2,Mistral,Mistral training dataset,27.50%,33.50%,6.40%,20.10%
DeepSeek R1 Distill Llama 70B,DeepSeek,DeepSeek training dataset,26.60%,27.60%,1.40%,19.70%
DeepSeek V3.2 Exp (Reasoning),DeepSeek,DeepSeek training dataset,78.90%,54.10%,29.10%,48.60%
DeepSeek R1 0528 Qwen3 8B,DeepSeek,DeepSeek training dataset,51.30%,19.90%,1.40%,24.40%
DeepSeek V3.2 Exp (Non-reasoning),DeepSeek,DeepSeek training dataset,55.40%,43.10%,23.40%,39.60%
DeepSeek R1 0528 (May '25),DeepSeek,DeepSeek training dataset,77.00%,39.60%,14.90%,44.10%
DeepSeek V3.1 Terminus (Non-reasoning),DeepSeek,DeepSeek training dataset,52.90%,41.20%,29.80%,38.30%
DeepSeek V3.1 Terminus (Reasoning),DeepSeek,DeepSeek training dataset,79.80%,57.00%,28.40%,49.60%
Grok 4 Fast (Non-reasoning),xAI,xAI training dataset,40.10%,37.70%,11.30%,28.10%
Grok Code Fast 1,xAI,xAI training dataset,65.70%,41.40%,16.30%,39.40%
Grok 4.1 Fast (Reasoning),xAI,xAI training dataset,82.20%,52.70%,22.70%,49.70%
Grok 4 Fast (Reasoning),xAI,xAI training dataset,83.20%,50.50%,17.70%,48.40%
Grok 4,xAI,xAI training dataset,81.90%,53.70%,37.60%,55.10%
Grok 3 mini Reasoning (high),xAI,xAI training dataset,69.60%,45.90%,16.30%,42.20%
Phi-4,Microsoft Azure,Microsoft Azure training dataset,23.10%,23.50%,3.50%,17.60%
Phi-4 Mini Instruct,Microsoft Azure,Microsoft Azure training dataset,12.60%,21.10%,0.00%,7.80%
Phi-4 Multimodal Instruct,Microsoft Azure,Microsoft Azure training dataset,13.10%,N/A,N/A,N/A
LFM2 1.2B,Liquid AI,Liquid AI training dataset,2.00%,22.00%,0.00%,1.50%
LFM2 2.6B,Liquid AI,Liquid AI training dataset,8.10%,19.50%,0.70%,3.80%
LFM2 8B A1B,Liquid AI,Liquid AI training dataset,15.10%,26.30%,0.00%,7.30%
Solar Pro 2 (Reasoning),Upstage,Upstage training dataset,61.60%,37.10%,2.80%,31.50%
Solar Pro 2 (Non-reasoning),Upstage,Upstage training dataset,42.40%,33.70%,4.30%,23.80%
MiniMax-Text-01,MiniMax,MiniMax training dataset,24.70%,32.70%,2.10%,17.30%
MiniMax-M2,MiniMax,MiniMax training dataset,82.60%,72.30%,24.10%,47.60%
Llama 3.1 Nemotron Instruct 70B,NVIDIA,NVIDIA training dataset,16.90%,30.70%,4.30%,14.80%
Llama Nemotron Super 49B v1.5 (Non-reasoning),NVIDIA,NVIDIA training dataset,29.00%,32.90%,3.50%,18.80%
Llama 3.3 Nemotron Super 49B v1 (Reasoning),NVIDIA,NVIDIA training dataset,27.70%,38.10%,0.00%,18.70%
Llama 3.3 Nemotron Super 49B v1 (Non-reasoning),NVIDIA,NVIDIA training dataset,28.00%,39.50%,0.00%,17.00%
Llama 3.1 Nemotron Ultra 253B v1 (Reasoning),NVIDIA,NVIDIA training dataset,64.10%,38.20%,2.10%,33.70%
NVIDIA Nemotron Nano 9B V2 (Reasoning),NVIDIA,NVIDIA training dataset,72.40%,27.60%,1.40%,31.90%
NVIDIA Nemotron Nano 9B V2 (Non-reasoning),NVIDIA,NVIDIA training dataset,70.10%,27.10%,0.70%,30.60%
Llama 3.1 Nemotron Nano 4B v1.1 (Reasoning),NVIDIA,NVIDIA training dataset,49.30%,25.50%,N/A,N/A
Llama Nemotron Super 49B v1.5 (Reasoning),NVIDIA,NVIDIA training dataset,73.70%,37.00%,5.00%,37.80%
Kimi K2 Thinking,Moonshot AI,Moonshot AI training dataset,85.30%,68.10%,29.10%,52.20%
Kimi K2 0905,Moonshot AI,Moonshot AI training dataset,61.00%,41.70%,22.70%,38.10%
Kimi Linear 48B A3B Instruct,Moonshot AI,Moonshot AI training dataset,37.80%,28.10%,10.60%,22.80%
OLMo 2 32B,Allen Institute for AI,Allen Institute for AI training dataset,6.80%,38.10%,0.00%,4.90%
OLMo 2 7B,Allen Institute for AI,Allen Institute for AI training dataset,4.10%,24.40%,0.00%,2.60%
Molmo 7B-D,Allen Institute for AI,Allen Institute for AI training dataset,3.90%,19.70%,0.00%,2.50%
Granite 4.0 H 350M,IBM,IBM training dataset,1.90%,17.60%,0.00%,1.20%
Granite 4.0 350M,IBM,IBM training dataset,2.40%,15.90%,0.00%,1.10%
Granite 4.0 H 1B,IBM,IBM training dataset,11.50%,26.20%,0.00%,6.60%
Granite 4.0 1B,IBM,IBM training dataset,4.70%,20.50%,0.00%,4.50%
Granite 4.0 H Small,IBM,IBM training dataset,25.10%,31.50%,2.10%,16.10%
Granite 4.0 Micro,IBM,IBM training dataset,18.00%,24.80%,1.40%,10.40%
DeepHermes 3 - Mistral 24B Preview (Non-reasoning),Nous Research,Nous Research training dataset,19.50%,N/A,N/A,N/A
Hermes 4 - Llama-3.1 70B (Reasoning),Nous Research,Nous Research training dataset,65.30%,31.30%,4.30%,34.60%
DeepHermes 3 - Llama-3.1 8B Preview (Non-reasoning),Nous Research,Nous Research training dataset,8.50%,N/A,N/A,N/A
Hermes 4 - Llama-3.1 405B (Non-reasoning),Nous Research,Nous Research training dataset,54.60%,34.80%,9.20%,32.80%
Hermes 4 - Llama-3.1 70B (Non-reasoning),Nous Research,Nous Research training dataset,26.90%,29.00%,0.00%,18.20%
Hermes 4 - Llama-3.1 405B (Reasoning),Nous Research,Nous Research training dataset,68.60%,32.70%,10.60%,34.80%
EXAONE 4.0 32B (Non-reasoning),LG AI Research,LG AI Research training dataset,47.20%,33.50%,1.40%,24.60%
Exaone 4.0 1.2B (Non-reasoning),LG AI Research,LG AI Research training dataset,29.30%,25.30%,0.00%,12.20%
EXAONE 4.0 32B (Reasoning),LG AI Research,LG AI Research training dataset,74.70%,36.30%,3.50%,37.50%
Exaone 4.0 1.2B (Reasoning),LG AI Research,LG AI Research training dataset,51.60%,23.00%,0.00%,20.30%
ERNIE 4.5 300B A47B,Baidu,Baidu training dataset,46.70%,39.10%,5.70%,27.90%
Cogito v2.1 (Reasoning),Deep Cogito,Deep Cogito training dataset,68.80%,46.30%,15.60%,41.80%
GLM-4.5-Air,Z AI,Z AI training dataset,68.40%,37.60%,19.10%,39.40%
GLM-4.6 (Reasoning),Z AI,Z AI training dataset,69.50%,43.40%,23.40%,43.80%
GLM-4.5V (Reasoning),Z AI,Z AI training dataset,60.40%,34.20%,5.00%,29.20%
GLM-4.6 (Non-reasoning),Z AI,Z AI training dataset,56.10%,36.70%,27.00%,38.70%
GLM-4.5V (Non-reasoning),Z AI,Z AI training dataset,35.20%,28.60%,6.40%,20.10%
Aya Expanse 32B,Cohere,Cohere training dataset,13.70%,28.80%,0.70%,9.80%
Aya Expanse 8B,Cohere,Cohere training dataset,7.00%,24.10%,0.00%,4.90%
Command A,Cohere,Cohere training dataset,28.70%,36.50%,0.70%,19.20%
Apriel-v1.5-15B-Thinker,ServiceNow,ServiceNow training dataset,72.80%,61.70%,9.90%,39.20%
Jamba 1.7 Large,AI21 Labs,AI21 Labs training dataset,18.10%,35.20%,2.10%,13.00%
Jamba Reasoning 3B,AI21 Labs,AI21 Labs training dataset,21.00%,52.40%,0.70%,9.20%
Jamba 1.7 Mini,AI21 Labs,AI21 Labs training dataset,6.10%,31.40%,0.00%,5.10%
Qwen3 Coder 480B A35B Instruct,Alibaba,Alibaba training dataset,58.50%,40.50%,17.70%,37.40%
Qwen3 4B 2507 Instruct,Alibaba,Alibaba training dataset,37.70%,33.50%,4.30%,20.00%
Qwen3 4B 2507 (Reasoning),Alibaba,Alibaba training dataset,64.10%,49.80%,1.40%,30.40%
Qwen3 235B A22B 2507 Instruct,Alibaba,Alibaba training dataset,52.40%,46.10%,14.20%,34.20%
Qwen3 Coder 30B A3B Instruct,Alibaba,Alibaba training dataset,40.30%,32.70%,14.20%,27.40%
Qwen3 VL 32B (Reasoning),Alibaba,Alibaba training dataset,73.80%,59.40%,7.10%,36.40%
Qwen3 VL 32B Instruct,Alibaba,Alibaba training dataset,51.40%,39.20%,7.80%,29.80%
Qwen3 235B A22B 2507 (Reasoning),Alibaba,Alibaba training dataset,78.80%,51.20%,12.80%,44.60%
Qwen3 Next 80B A3B Instruct,Alibaba,Alibaba training dataset,68.40%,39.70%,7.10%,35.40%
Qwen3 Next 80B A3B (Reasoning),Alibaba,Alibaba training dataset,78.40%,60.70%,9.20%,42.10%
Qwen3 30B A3B 2507 (Reasoning),Alibaba,Alibaba training dataset,70.70%,50.70%,5.00%,36.30%
Qwen3 30B A3B 2507 Instruct,Alibaba,Alibaba training dataset,51.50%,33.10%,5.70%,29.20%
Qwen3 Omni 30B A3B (Reasoning),Alibaba,Alibaba training dataset,67.90%,43.40%,3.50%,34.00%
Qwen3 VL 235B A22B Instruct,Alibaba,Alibaba training dataset,59.40%,42.70%,6.40%,33.90%
Qwen3 Omni 30B A3B Instruct,Alibaba,Alibaba training dataset,42.20%,31.20%,1.40%,20.80%
Qwen3 VL 30B A3B Instruct,Alibaba,Alibaba training dataset,47.60%,33.10%,5.70%,28.00%
Qwen3 VL 30B A3B (Reasoning),Alibaba,Alibaba training dataset,69.70%,45.10%,5.00%,34.50%
Qwen3 Max,Alibaba,Alibaba training dataset,76.70%,44.10%,19.10%,44.70%
Qwen3 VL 235B A22B (Reasoning),Alibaba,Alibaba training dataset,64.60%,56.50%,10.60%,38.40%
Qwen3 Max Thinking,Alibaba,Alibaba training dataset,53.50%,53.80%,16.30%,36.20%
Qwen3 VL 4B Instruct,Alibaba,Alibaba training dataset,29.00%,31.80%,0.00%,14.20%
Qwen3 VL 8B Instruct,Alibaba,Alibaba training dataset,33.20%,32.30%,2.10%,17.60%
Qwen3 VL 8B (Reasoning),Alibaba,Alibaba training dataset,35.30%,39.90%,3.50%,20.30%
Qwen3 VL 4B (Reasoning),Alibaba,Alibaba training dataset,32.00%,36.60%,1.40%,16.80%
Ring-flash-2.0,InclusionAI,InclusionAI training dataset,62.80%,43.30%,7.10%,28.90%
Ring-1T,InclusionAI,InclusionAI training dataset,64.30%,44.60%,6.40%,35.80%
Ling-mini-2.0,InclusionAI,InclusionAI training dataset,42.90%,23.60%,0.70%,19.00%
Ling-flash-2.0,InclusionAI,InclusionAI training dataset,58.90%,34.40%,9.90%,32.60%
Ling-1T,InclusionAI,InclusionAI training dataset,67.70%,34.80%,9.90%,37.60%
Seed-OSS-36B-Instruct,ByteDance Seed,ByteDance Seed training dataset,76.50%,41.90%,6.40%,39.80%
Doubao Seed Code,ByteDance Seed,ByteDance Seed training dataset,76.60%,51.40%,24.80%,47.40%
o1,OpenAI,OpenAI training dataset,67.90%,N/A,12.10%,38.60%
o1-preview,OpenAI,OpenAI training dataset,N/A,N/A,N/A,34.00%
o1-mini,OpenAI,OpenAI training dataset,57.60%,N/A,N/A,N/A
GPT-4o (Aug '24),OpenAI,OpenAI training dataset,31.70%,N/A,N/A,N/A
GPT-4o (May '24),OpenAI,OpenAI training dataset,33.40%,N/A,N/A,24.20%
GPT-4 Turbo,OpenAI,OpenAI training dataset,29.10%,N/A,N/A,21.50%
GPT-4o (Nov '24),OpenAI,OpenAI training dataset,30.90%,34.30%,7.80%,24.00%
GPT-4o mini,OpenAI,OpenAI training dataset,23.40%,31.00%,N/A,N/A
GPT-3.5 Turbo,OpenAI,OpenAI training dataset,N/A,N/A,N/A,10.70%
GPT-4.1,OpenAI,OpenAI training dataset,45.70%,43.00%,12.80%,32.20%
o3-mini (high),OpenAI,OpenAI training dataset,73.40%,N/A,N/A,42.10%
GPT-4.1 nano,OpenAI,OpenAI training dataset,32.60%,32.00%,3.50%,20.70%
GPT-4.1 mini,OpenAI,OpenAI training dataset,48.30%,38.30%,7.10%,31.90%
o4-mini (high),OpenAI,OpenAI training dataset,85.90%,68.70%,14.20%,48.90%
"GPT-4o (March 2025, chatgpt-4o-latest)",OpenAI,OpenAI training dataset,42.50%,N/A,N/A,N/A
o3-mini,OpenAI,OpenAI training dataset,71.70%,N/A,6.40%,39.40%
GPT-4,OpenAI,OpenAI training dataset,N/A,N/A,N/A,13.10%
Llama 3.1 Instruct 70B,Meta,Meta training dataset,23.20%,34.40%,2.80%,17.60%
Llama 3.1 Instruct 8B,Meta,Meta training dataset,11.60%,28.60%,0.70%,8.50%
Llama 3.2 Instruct 3B,Meta,Meta training dataset,8.30%,26.20%,N/A,N/A
Llama 3 Instruct 70B,Meta,Meta training dataset,19.80%,N/A,N/A,N/A
Llama 3 Instruct 8B,Meta,Meta training dataset,9.60%,N/A,N/A,N/A
Llama 3.2 Instruct 1B,Meta,Meta training dataset,1.90%,22.80%,0.00%,1.20%
Llama 2 Chat 70B,Meta,Meta training dataset,9.80%,N/A,N/A,N/A
Llama 2 Chat 13B,Meta,Meta training dataset,9.80%,N/A,N/A,N/A
Llama 2 Chat 7B,Meta,Meta training dataset,0.20%,N/A,N/A,N/A
Gemini 2.0 Pro Experimental (Feb '25),Google,Google training dataset,34.70%,N/A,N/A,25.50%
Gemini 2.0 Flash (experimental),Google,Google training dataset,21.00%,N/A,N/A,N/A
Gemini 1.5 Pro (Sep '24),Google,Google training dataset,31.60%,N/A,N/A,23.60%
Gemini 2.0 Flash-Lite (Preview),Google,Google training dataset,17.90%,N/A,N/A,N/A
Gemini 2.0 Flash (Feb '25),Google,Google training dataset,33.40%,40.20%,3.50%,23.40%
Gemini 1.5 Flash (Sep '24),Google,Google training dataset,27.30%,N/A,N/A,N/A
Gemma 2 27B,Google,Google training dataset,27.90%,N/A,N/A,N/A
Gemma 2 9B,Google,Google training dataset,12.60%,N/A,N/A,N/A
Gemini 1.5 Flash-8B,Google,Google training dataset,21.70%,N/A,N/A,N/A
Gemini 2.0 Flash Thinking Experimental (Jan '25),Google,Google training dataset,32.10%,N/A,N/A,24.10%
Gemini 2.5 Flash-Lite (Reasoning),Google,Google training dataset,59.30%,49.90%,4.30%,27.60%
Gemini 1.0 Pro,Google,Google training dataset,11.60%,N/A,N/A,N/A
Gemini 1.5 Pro (May '24),Google,Google training dataset,24.40%,N/A,N/A,19.80%
Gemini 1.0 Ultra,Google,Google training dataset,N/A,N/A,N/A,17.60%
Gemini 2.5 Flash Preview (Non-reasoning),Google,Google training dataset,40.60%,N/A,N/A,N/A
Gemini 1.5 Flash (May '24),Google,Google training dataset,19.60%,N/A,N/A,N/A
Gemini 2.5 Pro Preview (May' 25),Google,Google training dataset,77.00%,N/A,N/A,N/A
Gemini 2.5 Flash (Non-reasoning),Google,Google training dataset,49.50%,39.00%,11.30%,30.00%
Gemini 2.5 Flash Preview (Reasoning),Google,Google training dataset,50.50%,N/A,N/A,N/A
Gemini 2.0 Flash-Lite (Feb '25),Google,Google training dataset,18.50%,N/A,N/A,N/A
Gemini 2.5 Flash (Reasoning),Google,Google training dataset,69.50%,50.30%,12.80%,40.50%
Gemini 2.5 Flash-Lite (Non-reasoning),Google,Google training dataset,40.00%,31.50%,2.10%,19.90%
Gemma 3n E4B Instruct Preview (May '25),Google,Google training dataset,13.80%,N/A,N/A,N/A
Gemini 2.5 Pro Preview (Mar' 25),Google,Google training dataset,77.80%,N/A,N/A,46.70%
PALM-2,Google,Google training dataset,N/A,N/A,N/A,4.60%
Mistral Large 2 (Nov '24),Mistral,Mistral training dataset,29.30%,31.20%,5.70%,21.40%
Mistral Large 2 (Jul '24),Mistral,Mistral training dataset,26.70%,31.60%,N/A,N/A
Pixtral Large,Mistral,Mistral training dataset,26.10%,34.50%,N/A,N/A
Mistral Small 3,Mistral,Mistral training dataset,25.20%,26.40%,N/A,N/A
Mistral Small (Sep '24),Mistral,Mistral training dataset,14.10%,N/A,N/A,N/A
Mixtral 8x22B Instruct,Mistral,Mistral training dataset,14.80%,N/A,N/A,N/A
Mistral Small (Feb '24),Mistral,Mistral training dataset,11.10%,N/A,N/A,N/A
Mistral Large (Feb '24),Mistral,Mistral training dataset,17.80%,N/A,N/A,N/A
Pixtral 12B (2409),Mistral,Mistral training dataset,11.50%,N/A,N/A,N/A
Mistral NeMo,Mistral,Mistral training dataset,5.70%,N/A,N/A,N/A
Mixtral 8x7B Instruct,Mistral,Mistral training dataset,6.60%,N/A,N/A,N/A
Codestral-Mamba,Mistral,Mistral training dataset,13.30%,N/A,N/A,N/A
Mistral 7B Instruct,Mistral,Mistral training dataset,4.60%,N/A,N/A,N/A
Devstral Small (May '25),Mistral,Mistral training dataset,25.80%,N/A,N/A,N/A
Mistral Small 3.1,Mistral,Mistral training dataset,21.20%,29.90%,N/A,N/A
Codestral (May '24),Mistral,Mistral training dataset,21.30%,N/A,N/A,N/A
Mistral Medium,Mistral,Mistral training dataset,9.90%,N/A,N/A,N/A
Magistral Small 1,Mistral,Mistral training dataset,51.40%,24.80%,4.30%,26.60%
Magistral Medium 1,Mistral,Mistral training dataset,52.70%,25.10%,8.50%,30.30%
Mistral Medium 3,Mistral,Mistral training dataset,40.00%,39.30%,3.50%,25.60%
DeepSeek R1 Distill Qwen 32B,DeepSeek,DeepSeek training dataset,27.00%,22.90%,N/A,N/A
DeepSeek V3 (Dec '24),DeepSeek,DeepSeek training dataset,35.90%,34.80%,6.40%,25.90%
DeepSeek R1 Distill Qwen 14B,DeepSeek,DeepSeek training dataset,37.60%,22.10%,N/A,N/A
DeepSeek R1 Distill Llama 8B,DeepSeek,DeepSeek training dataset,23.30%,17.60%,N/A,N/A
DeepSeek R1 Distill Qwen 1.5B,DeepSeek,DeepSeek training dataset,7.00%,13.20%,N/A,N/A
DeepSeek V3.1 (Non-reasoning),DeepSeek,DeepSeek training dataset,57.70%,37.80%,22.70%,39.00%
DeepSeek R1 (Jan '25),DeepSeek,DeepSeek training dataset,61.70%,39.00%,5.70%,34.40%
DeepSeek V3.1 (Reasoning),DeepSeek,DeepSeek training dataset,78.40%,41.50%,24.10%,47.20%
DeepSeek V3 0324,DeepSeek,DeepSeek training dataset,40.50%,41.00%,14.20%,30.20%
DeepSeek Coder V2 Lite Instruct,DeepSeek,DeepSeek training dataset,15.80%,N/A,N/A,N/A
Sonar Pro,Perplexity,Perplexity training dataset,27.50%,N/A,N/A,N/A
Sonar,Perplexity,Perplexity training dataset,29.50%,N/A,N/A,N/A
Grok Beta,xAI,xAI training dataset,24.10%,N/A,N/A,N/A
Grok 3,xAI,xAI training dataset,42.50%,46.90%,10.60%,30.00%
Grok 2 (Dec '24),xAI,xAI training dataset,26.70%,N/A,N/A,N/A
OpenChat 3.5 (1210),OpenChat,OpenChat training dataset,11.50%,N/A,N/A,N/A
Phi-3 Medium Instruct 14B,Microsoft Azure,Microsoft Azure training dataset,15.00%,21.50%,0.00%,8.90%
Phi-3 Mini Instruct 3.8B,Microsoft Azure,Microsoft Azure training dataset,11.60%,23.90%,0.00%,6.90%
LFM 40B,Liquid AI,Liquid AI training dataset,9.60%,N/A,N/A,N/A
Solar Pro 2 (Preview) (Non-reasoning),Upstage,Upstage training dataset,38.50%,N/A,N/A,N/A
Solar Pro 2 (Preview) (Reasoning),Upstage,Upstage training dataset,46.20%,N/A,N/A,N/A
DBRX Instruct,Databricks,Databricks training dataset,9.30%,N/A,N/A,N/A
MiniMax M1 40k,MiniMax,MiniMax training dataset,65.70%,41.20%,2.10%,35.20%
MiniMax M1 80k,MiniMax,MiniMax training dataset,71.10%,41.80%,2.80%,37.10%
Kimi K2,Moonshot AI,Moonshot AI training dataset,55.60%,41.50%,14.90%,35.00%
Llama 3.1 Tulu3 405B,Allen Institute for AI,Allen Institute for AI training dataset,29.10%,N/A,N/A,N/A
Granite 3.3 8B (Non-reasoning),IBM,IBM training dataset,12.70%,22.40%,0.00%,7.60%
Hermes 3 - Llama-3.1 70B,Nous Research,Nous Research training dataset,18.80%,N/A,N/A,N/A
GLM-4.5 (Reasoning),Z AI,Z AI training dataset,73.80%,44.10%,21.30%,43.30%
Command-R+ (Aug '24),Cohere,Cohere training dataset,11.10%,N/A,N/A,N/A
Command-R+ (Apr '24),Cohere,Cohere training dataset,12.20%,N/A,N/A,N/A
Command-R (Aug '24),Cohere,Cohere training dataset,4.40%,N/A,N/A,N/A
Command-R (Mar '24),Cohere,Cohere training dataset,4.80%,N/A,N/A,N/A
Jamba Instruct,AI21 Labs,AI21 Labs training dataset,4.60%,N/A,N/A,N/A
Jamba 1.6 Mini,AI21 Labs,AI21 Labs training dataset,7.10%,N/A,N/A,N/A
Jamba 1.6 Large,AI21 Labs,AI21 Labs training dataset,17.20%,N/A,N/A,N/A
Jamba 1.5 Mini,AI21 Labs,AI21 Labs training dataset,6.20%,N/A,N/A,N/A
Jamba 1.5 Large,AI21 Labs,AI21 Labs training dataset,14.30%,N/A,N/A,N/A
Qwen2.5 Max,Alibaba,Alibaba training dataset,35.90%,N/A,N/A,N/A
Qwen2.5 Instruct 72B,Alibaba,Alibaba training dataset,27.60%,36.90%,4.30%,19.50%
Qwen2.5 Coder Instruct 32B,Alibaba,Alibaba training dataset,29.50%,N/A,N/A,N/A
Qwen2.5 Turbo,Alibaba,Alibaba training dataset,16.30%,N/A,N/A,N/A
Qwen2 Instruct 72B,Alibaba,Alibaba training dataset,15.90%,N/A,N/A,N/A
Qwen3 32B (Non-reasoning),Alibaba,Alibaba training dataset,28.80%,31.50%,N/A,N/A
Qwen3 4B (Non-reasoning),Alibaba,Alibaba training dataset,23.30%,N/A,N/A,N/A
Qwen2.5 Instruct 32B,Alibaba,Alibaba training dataset,24.80%,N/A,N/A,N/A
Qwen3 30B A3B (Reasoning),Alibaba,Alibaba training dataset,50.60%,41.50%,2.10%,27.10%
Qwen3 235B A22B (Reasoning),Alibaba,Alibaba training dataset,62.20%,38.70%,5.70%,35.90%
Qwen3 32B (Reasoning),Alibaba,Alibaba training dataset,54.60%,36.30%,2.80%,30.90%
Qwen3 14B (Non-reasoning),Alibaba,Alibaba training dataset,28.00%,23.90%,5.00%,19.80%
Qwen3 1.7B (Non-reasoning),Alibaba,Alibaba training dataset,12.60%,21.10%,0.00%,6.50%
Qwen3 8B (Non-reasoning),Alibaba,Alibaba training dataset,20.20%,28.60%,2.10%,13.00%
Qwen3 8B (Reasoning),Alibaba,Alibaba training dataset,40.60%,33.50%,2.10%,21.80%
QwQ 32B,Alibaba,Alibaba training dataset,63.10%,38.80%,N/A,N/A
Qwen3 235B A22B (Non-reasoning),Alibaba,Alibaba training dataset,34.30%,36.60%,5.70%,23.30%
QwQ 32B-Preview,Alibaba,Alibaba training dataset,33.70%,N/A,N/A,N/A
Qwen3 4B (Reasoning),Alibaba,Alibaba training dataset,46.50%,32.50%,N/A,N/A
Qwen3 0.6B (Non-reasoning),Alibaba,Alibaba training dataset,7.30%,21.90%,0.00%,3.80%
Qwen3 30B A3B (Non-reasoning),Alibaba,Alibaba training dataset,32.20%,31.90%,6.40%,21.60%
Qwen2.5 Coder Instruct 7B ,Alibaba,Alibaba training dataset,12.60%,N/A,N/A,N/A
Qwen3 14B (Reasoning),Alibaba,Alibaba training dataset,52.30%,40.50%,3.50%,29.10%
Qwen3 1.7B (Reasoning),Alibaba,Alibaba training dataset,30.80%,26.90%,0.00%,11.70%
Qwen3 Max (Preview),Alibaba,Alibaba training dataset,65.10%,48.00%,18.40%,40.20%
Qwen3 0.6B (Reasoning),Alibaba,Alibaba training dataset,12.10%,23.30%,0.00%,5.00%
